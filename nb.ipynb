{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  article dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as Datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as tqdm_regular\n",
    "import seaborn as sns\n",
    "from torchvision.utils import make_grid\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  configuring device\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load in the CIFAR10 dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  loading training data\n",
    "training_set = Datasets.CIFAR10(root='./', download=True,\n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "#  loading validation data\n",
    "validation_set = Datasets.CIFAR10(root='./', download=True, train=False,\n",
    "                                  transform=transforms.ToTensor())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract classes "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_each_class(dataset):\n",
    "  \"\"\"\n",
    "  This function searches for and returns\n",
    "  one image per class\n",
    "  \"\"\"\n",
    "  images = []\n",
    "  ITERATE = True\n",
    "  i = 0\n",
    "  j = 0\n",
    "\n",
    "  while ITERATE:\n",
    "    for label in tqdm_regular(dataset.targets):\n",
    "      if label==j:\n",
    "        images.append(dataset.data[i])\n",
    "        print(f'class {j} found')\n",
    "        i+=1\n",
    "        j+=1\n",
    "        if j==10:\n",
    "          ITERATE = False\n",
    "      else:\n",
    "        i+=1\n",
    "\n",
    "  return images\n",
    "  \n",
    "  \n",
    "#  extracting training images\n",
    "training_images = [x for x in training_set.data]\n",
    "\n",
    "#  extracting validation images\n",
    "validation_images = [x for x in validation_set.data]\n",
    "\n",
    "#  extracting one image from each class in the validation set\n",
    "test_images = extract_each_class(validation_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grayscale & normalize the images"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  converting images to grayscale by taking mean across axis-2 (depth)\n",
    "training_gray = [x.mean(axis=2) for x in training_images]\n",
    "validation_gray = [x.mean(axis=2) for x in validation_images]\n",
    "test_gray = [x.mean(axis=2) for x in test_images]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def min_max_normalize_gray(dataset: list):\n",
    "  \"\"\"\n",
    "  This function normalizes data by constraining\n",
    "  data points between the range of 0 & 1  \n",
    "  \"\"\"\n",
    "  #  create a list to hold normalized data  \n",
    "  normalized = []\n",
    "\n",
    "  for image in tqdm_regular(dataset):\n",
    "    #  creating temporary store\n",
    "    temp = []\n",
    "    #  flatenning\n",
    "    pixels = image.flatten()\n",
    "    #  derive minimum and maximum values\n",
    "    minimum = pixels.min()\n",
    "    maximum = pixels.max()\n",
    "    #  convert to list for iteration\n",
    "    pixels = list(pixels)\n",
    "\n",
    "    for pixel in pixels:\n",
    "      #  normalizing pixels\n",
    "      normalize = (pixel-minimum)/(maximum-minimum)\n",
    "      #  appending each pixel to temporary store\n",
    "      temp.append(round(normalize, 2))\n",
    "\n",
    "    temp = np.array(temp)\n",
    "    temp = temp.reshape((32, 32))\n",
    "    #  appending normalized image to list\n",
    "    normalized.append(temp)\n",
    "  \n",
    "  return normalized\n",
    "  \n",
    "  \n",
    "#  normalizing pixels\n",
    "training_gray = min_max_normalize_gray(training_gray)\n",
    "validation_gray = min_max_normalize_gray(validation_gray)\n",
    "test_gray = min_max_normalize_gray(test_gray)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating noisy copies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def random_noise(dataset: list, noise_intensity=0.2):\n",
    "  \"\"\"\n",
    "  This function replicates the salt and pepper noise process\n",
    "  \"\"\"\n",
    "  noised = []\n",
    "  noise_threshold = 1 - noise_intensity\n",
    "\n",
    "  for image in tqdm_regular(dataset):\n",
    "    #  flatenning image\n",
    "    image = image.reshape(1024)\n",
    "\n",
    "    #  creating vector of zeros\n",
    "    noise_vector = np.zeros(1024)\n",
    "\n",
    "    #  noise probability\n",
    "    for idx in range(1024):\n",
    "      regulator = round(random.random(), 1)\n",
    "      if regulator > noise_threshold:\n",
    "        noise_vector[idx] = 1\n",
    "      elif regulator == noise_threshold:\n",
    "        noise_vector[idx] = 0\n",
    "      else:\n",
    "        noise_vector[idx] = image[idx]\n",
    "    \n",
    "    #  reshaping noise vectors\n",
    "    noise_vector = noise_vector.reshape((32, 32))\n",
    "\n",
    "    noised.append(noise_vector)\n",
    "  return noised\n",
    "  \n",
    "  \n",
    "#  adding noise to images\n",
    "training_noised = random_noise(training_gray)\n",
    "validation_noised = random_noise(validation_gray)\n",
    "test_noised = random_noise(test_gray)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  creating image-target pair\n",
    "training_set = list(zip(training_noised, training_gray))\n",
    "validation_set = list(zip(validation_noised, validation_gray))\n",
    "test_set = list(zip(test_noised, test_gray))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the pytorch dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  defining dataset class\n",
    "class CustomCIFAR10(Dataset):\n",
    "  def __init__(self, data, transforms=None):\n",
    "    self.data = data\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image = self.data[idx][0]\n",
    "    target = self.data[idx][1]\n",
    "\n",
    "    if self.transforms!=None:\n",
    "      image = self.transforms(image)\n",
    "      target = self.transforms(target)\n",
    "    return (image, target)\n",
    "    \n",
    "    \n",
    "#  creating pytorch datasets\n",
    "training_data = CustomCIFAR10(training_set, transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                          transforms.Normalize(0.5, 0.5)]))\n",
    "validation_data = CustomCIFAR10(validation_set, transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                              transforms.Normalize(0.5, 0.5)]))\n",
    "test_data = CustomCIFAR10(test_set, transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                  transforms.Normalize(0.5, 0.5)]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the encoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  defining encoder\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, in_channels=3, out_channels=16, latent_dim=1000, act_fn=nn.ReLU()):\n",
    "    super().__init__()\n",
    "    self.in_channels = in_channels\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1), # (32, 32)\n",
    "        act_fn,\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1), \n",
    "        act_fn,\n",
    "        nn.Conv2d(out_channels, 2*out_channels, 3, padding=1, stride=2), # (16, 16)\n",
    "        act_fn,\n",
    "        nn.Conv2d(2*out_channels, 2*out_channels, 3, padding=1),\n",
    "        act_fn,\n",
    "        nn.Conv2d(2*out_channels, 4*out_channels, 3, padding=1, stride=2), # (8, 8)\n",
    "        act_fn,\n",
    "        nn.Conv2d(4*out_channels, 4*out_channels, 3, padding=1),\n",
    "        act_fn,\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(4*out_channels*8*8, latent_dim),\n",
    "        act_fn\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(-1, self.in_channels, 32, 32)\n",
    "    output = self.net(x)\n",
    "    return output\n",
    "\n",
    "\n",
    "#  defining decoder\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, in_channels=3, out_channels=16, latent_dim=1000, act_fn=nn.ReLU()):\n",
    "    super().__init__()\n",
    "\n",
    "    self.out_channels = out_channels\n",
    "\n",
    "    self.linear = nn.Sequential(\n",
    "        nn.Linear(latent_dim, 4*out_channels*8*8),\n",
    "        act_fn\n",
    "    )\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.ConvTranspose2d(4*out_channels, 4*out_channels, 3, padding=1), # (8, 8)\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(4*out_channels, 2*out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1), # (16, 16)\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(2*out_channels, 2*out_channels, 3, padding=1),\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(2*out_channels, out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1), # (32, 32)\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(out_channels, out_channels, 3, padding=1),\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(out_channels, in_channels, 3, padding=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.linear(x)\n",
    "    output = output.view(-1, 4*self.out_channels, 8, 8)\n",
    "    output = self.conv(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "#  defining autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.encoder.to(device)\n",
    "\n",
    "    self.decoder = decoder\n",
    "    self.decoder.to(device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolutional autoencoder class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  defining class\n",
    "class ConvolutionalAutoencoder():\n",
    "  def __init__(self, autoencoder):\n",
    "    self.network = autoencoder\n",
    "    self.optimizer = torch.optim.Adam(self.network.parameters(), lr=1e-3)\n",
    "\n",
    "  def train(self, loss_function, epochs, batch_size, \n",
    "            training_set, validation_set, test_set,\n",
    "            image_channels=3):\n",
    "    \n",
    "    #  creating log\n",
    "    log_dict = {\n",
    "        'training_loss_per_batch': [],\n",
    "        'validation_loss_per_batch': [],\n",
    "        'visualizations': []\n",
    "    } \n",
    "\n",
    "    #  defining weight initialization function\n",
    "    def init_weights(module):\n",
    "      if isinstance(module, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "      elif isinstance(module, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "\n",
    "    #  initializing network weights\n",
    "    self.network.apply(init_weights)\n",
    "\n",
    "    #  creating dataloaders\n",
    "    train_loader = DataLoader(training_set, batch_size)\n",
    "    val_loader = DataLoader(validation_set, batch_size)\n",
    "    test_loader = DataLoader(test_set, 10)\n",
    "\n",
    "    #  setting convnet to training mode\n",
    "    self.network.train()\n",
    "    self.network.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      print(f'Epoch {epoch+1}/{epochs}')\n",
    "      train_losses = []\n",
    "\n",
    "      #------------\n",
    "      #  TRAINING\n",
    "      #------------\n",
    "      print('training...')\n",
    "      for images, targets in tqdm(train_loader):\n",
    "        #  zeroing gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        #  sending images and targets to device\n",
    "        images = images.to(device).type(torch.cuda.FloatTensor)\n",
    "        targets = targets.to(device).type(torch.cuda.FloatTensor)\n",
    "        #  reconstructing images\n",
    "        output = self.network(images)\n",
    "        #  computing loss\n",
    "        loss = loss_function(output, targets)\n",
    "        loss = loss#.type(torch.cuda.FloatTensor)\n",
    "        #  calculating gradients\n",
    "        loss.backward()\n",
    "        #  optimizing weights\n",
    "        self.optimizer.step()\n",
    "\n",
    "        #--------------\n",
    "        # LOGGING\n",
    "        #--------------\n",
    "        log_dict['training_loss_per_batch'].append(loss.item())\n",
    "\n",
    "      #--------------\n",
    "      # VALIDATION\n",
    "      #--------------\n",
    "      print('validating...')\n",
    "      for val_images, val_targets in tqdm(val_loader):\n",
    "        with torch.no_grad():\n",
    "          #  sending validation images and targets to device\n",
    "          val_images = val_images.to(device).type(torch.cuda.FloatTensor)\n",
    "          val_targets = val_targets.to(device).type(torch.cuda.FloatTensor)\n",
    "          #  reconstructing images\n",
    "          output = self.network(val_images)\n",
    "          #  computing validation loss\n",
    "          val_loss = loss_function(output, val_targets)\n",
    "\n",
    "        #--------------\n",
    "        # LOGGING\n",
    "        #--------------\n",
    "        log_dict['validation_loss_per_batch'].append(val_loss.item())\n",
    "\n",
    "\n",
    "      #--------------\n",
    "      # VISUALISATION\n",
    "      #--------------\n",
    "      print(f'training_loss: {round(loss.item(), 4)} validation_loss: {round(val_loss.item(), 4)}')\n",
    "\n",
    "      for test_images, test_targets in test_loader:\n",
    "        #  sending test images to device\n",
    "        test_images = test_images.to(device).type(torch.cuda.FloatTensor)\n",
    "        with torch.no_grad():\n",
    "          #  reconstructing test images\n",
    "          reconstructed_imgs = self.network(test_images)\n",
    "        #  sending reconstructed and images to cpu to allow for visualization\n",
    "        reconstructed_imgs = reconstructed_imgs.cpu()\n",
    "        test_images = test_images.cpu()\n",
    "\n",
    "        #  visualisation\n",
    "        imgs = torch.stack([test_images.view(-1, image_channels, 32, 32), reconstructed_imgs], \n",
    "                          dim=1).flatten(0,1)\n",
    "        grid = make_grid(imgs, nrow=10, normalize=True, padding=1)\n",
    "        grid = grid.permute(1, 2, 0)\n",
    "        plt.figure(dpi=170)\n",
    "        plt.title('Original/Reconstructed')\n",
    "        plt.imshow(grid)\n",
    "        log_dict['visualizations'].append(grid)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "      \n",
    "    return log_dict\n",
    "\n",
    "  def autoencode(self, x):\n",
    "    return self.network(x)\n",
    "\n",
    "  def encode(self, x):\n",
    "    encoder = self.network.encoder\n",
    "    return encoder(x)\n",
    "  \n",
    "  def decode(self, x):\n",
    "    decoder = self.network.decoder\n",
    "    return decoder(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a denoising autoencocer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  training model\n",
    "model = ConvolutionalAutoencoder(Autoencoder(Encoder(in_channels=1),\n",
    "                                               Decoder(in_channels=1)))\n",
    "\n",
    "log_dict = model.train(nn.MSELoss(), epochs=15, batch_size=64, \n",
    "                       training_set=training_data, validation_set=validation_data,\n",
    "                       test_set=test_data, image_channels=1)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}